---
title: "EDA & 이상치 탐지 및 제거"
subtitle: "공원일몰제와 수직빌딩숲 2편"
layout: post
author: "Minki"
header-img: "img/seoulpark_series/seoul_park.jpg"
header-mask: 0.6
# header-style: text
catalog: true
tags:
  - 공원일몰제
  - 파이썬
  - GIS
  - 딥러닝
  - 수직빌딩숲
---

*개인적인 호기심에서 진행한 연구입니다. 제가 잘못 알고 있는 점이나 수정&#8226;보완이 필요한 부분은 댓글로 알려주시면 감사드리겠습니다. 그럼 시작하겠습니다.*

<!-- <br> -->

# 들어가는 글

앞선 포스팅인 ['1편-공원일몰제'](https://minkithub.github.io/minki.github.io/%EB%8D%B0%EC%9D%B4%ED%84%B0%20%EC%B9%BC%EB%9F%BC/seoulpark/)에서 해당 제도가 가진 심각성과 그에 대한 해결책인 수직빌딩숲에 대해 살펴보았다. 앞선 포스팅을 보면 선정된 부지에 한해서 수직빌딩숲을 건설하는 것이 매입금액과 녹지 손실의 부담을 꽤 잘 줄일 수 있음을 느낄 수 있을 것이다. 그렇다면 수직빌딩숲을 지었을 때 얼마만큼의 미세먼지가 줄어들 수 있을까? 의문을 해소하기 위해 딥러닝과 수목의 미세먼지 저감 효과를 이용해서 수직빌딩숲의 미세먼지 저감 효과에 대해 알아보자.

<!-- <br> -->

# 모델 생성을 위한 데이터셋 구축

미세먼지를 예측하는 모델을 만들기 위해 우선 나는 LSTM 알고리즘을 사용했다. LSTM은 또다른 시계열 알고리즘인 RNN에서 Learning time과 Vanishing Gradient문제를 개선한 알고리즘으로 자세한 설명은 
다음 포스팅에서 하겠다. 모델을 만드는데 가장 먼저 해야할 일은 모델의 독립변수와 종속변수를 설정하는 일이다. 이미 종속변수는 미세먼지(PM10)로 결정이 됐으니, 올바른 독립변수를 골라 데이터셋을 구축해야 한다.

## 1. 미세먼지 독립변수 설정

![](/img/seoulpark_series/pollution.jpg)

미세먼지를 생성하는 물질은 '전구물질'이라 불린다. 그리고 이 전구물질에는 대표적으로 SO2, NO2, CO, O3 등 자동차와 공장의 매연, 그리고 석탄의 연소 시에 나오는 원소들이다. 실제로 전구물질간 화학작용으로 발생하는 미세먼지의 양은
전체 미세먼지 양의 2/3에 해당할 정도로 많은 양이다. 

또한 미세먼지는 계절성이 뚜렷하다. 이는 우리의 귀납적 관찰과 봄, 겨울만 되면 언론에서 쏟아내는 정보만 보더라도 알 수 있을 것이라 예상된다. 따라서 미세먼지 예측을 위한 독립변수는 우선 '전구물질' + '날씨 변수'로 선정했다.

* 기간 : 2010년 01월 01일 ~ 2018년 09월 30일 (3,414일)
* 전구물질 : SO2, CO, O3, NO2
* 기상 변수 : 평균 온도, 최고 온도, 최저 온도, 평균 습도, 최저 습도, 최고 습도, 평균 풍속, 최고 풍속, 강우량
* 종속 변수 : PM10

데이터셋은 2010년 1월 1일부터 2018년 9월 30일까지 길이가 3,414이며, 차원은 종속변수(PM10)는 1차원, 독립변수는 13차원이다.

<!-- <br> -->

# EDA(Exploratory Data Analysis)

EDA는 탐색적 데이터 분석을 일컫는 말이다. 데이터를 분석하기 전에 그래프나 통계적인 방법으로 자료를 직관적으로 바라봄으로써 분석에 대한 통찰을 얻을 수 있다.
미세먼지는 계절성이 매우 뚜렷한 데이터이기 때문에 EDA는 꼭 필요한 과정이다.

우선 내가 사용하는 데이터셋의 형태는 다음과 같다.
![](/img/seoulpark_series/dataset.png)


먼저 간단한 그래프를 통해 우리나라의 미세먼지 '매우 나쁨', '나쁨', '보통', '좋음'의 분포를 확인해보자.

```python
data['label'] = np.where(data['PM10']>150, '매우나쁨', 
np.where(data['PM10']>80, '나쁨', np.where(data['PM10']>30, '보통', '좋음')))
data['year'] = data['Date'].astype(str).apply(lambda x:x[:4])
```
위의 코드를 사용하여 미세먼지 라벨과 년도를 데이터셋에 칼럼으로 추가하였다. 이후 matplot 라이브러리를 사용하여 간단한 파이 차트를 그려보면 다음과 같다.

```python
f,ax=plt.subplots(1,2,figsize=(18,8))
data['label'].value_counts().plot.pie(explode=[0, 0, 0, 0.1],
autopct='%1.1f%%',ax=ax[0],shadow=False)
ax[0].set_title('Label')
ax[0].set_ylabel('Count')
sns.countplot('label', data=eda_data,ax=ax[1])
ax[1].set_title('Label')
plt.show()
```

![](/img/seoulpark_series/finedust_label.png)

위의 그래프를 통해 약 8년간 우리나라 날씨에서 미세먼지 '좋음'과 '보통'은 전체의 89.1%를 차지하고 있음을 알 수 있다. 그러나 최근들어 미세먼지가 심해지고 있다는 느낌을
지울수가 없었다. 그래서 이를 확인해보기 위해 연도별 미세먼지 라벨링의 변화를 살펴보았다.

```python
pd.crosstab(data.label,data.year,margins=True).style.background_gradient(cmap='summer_r')
```

![](/img/seoulpark_series/finedust_label_year.png)

![](/img/seoulpark_series/finedust_label_time.png)

2010년부터 2018년까지 미세먼지를 일별, 그리고 시간별로 라벨링 개수를 카운팅한 결과 2016년 이후 미세먼지 '매우나쁨' 일수와 시간이 늘어났다. 아마 이런 이유 때문에 사람들이
미세먼지가 과거보다 더 심해졌다는 인상을 받지 않았을까 싶다.

다음은 변수끼리 상관계수를 살펴보자.
```python
sns.heatmap(data.iloc[:, range(0, len(data.columns)-2)].corr(),annot=True,cmap='RdYlGn',linewidths=0.2,annot_kws={'size':20})
fig=plt.gcf()
fig.set_size_inches(18,15)
plt.xticks(fontsize=14)
plt.yticks(fontsize=14)
plt.show()
```
![](/img/seoulpark_series/cor.png)

미세먼지의 전구물질로 알려져있는 SO2, CO, O3, NO2 중에서는 O3를 제외하고 나머지는 미세먼지와 확실한 양의 상관관계를 보이고 있다. 또한 기상변수 중에서는
온도변수끼리 그리고 습도변수끼리 서로 높은 상관관계를 보이고 있다. 또한 온도 및 습도와 미세먼지는 약한 음의 관계를 띄고 있어, 여름철 미세먼지가 적은 이유를 설명해주고 있다.
그러나 예상외로 풍속과 강우량은 미세먼지와의 상관관계가 낮은 것으로 결과가 나왔다.

# 이상치(Outlier) 탐지 및 제거

이상치(outlier)란, 관측된 데이터 범위에서 매우 크거나 작은 값으로 데이터 범위를 많이 벗어난 값을 일컫는다. 딥러닝 혹은 머신러닝 모델을 만드는 과정에서 정규화는 매우 중요한 부분인데,
정규화 과정에서 데이터셋 내의 이상치가 제거되지 않으면 정규화 결과과 한쪽으로 편향될 수 있다. 그렇다면 이제 정규화를 해보자. 이번 정규화의 목적은 이상치를 탐지하는 과정이므로 독립변수인 PM10까지 포함하여
정규화를 진행하였다.

## Standard VS Robust

### 1. Standard Scale

평균은 0, 표준편차는 1이 되도록 변환해준다. 가장 일반적으로 사용되는 스케일링 기법이다.

```python
from sklearn.preprocessing import StandardScaler

scale_data = data.iloc[:, range(0, len(data.columns)-2)]

scaler = StandardScaler()
scaler.fit(scale_data)
scale_data_scaled_standard = scaler.transform(scale_data)
scale_data_scaled_standard = pd.DataFrame(scale_data_scaled_standard).describe()
scale_data_scaled_standard.columns = scale_data.columns
scale_data_scaled_standard = round(scale_data_scaled_standard)
```

![](/img/seoulpark_series/standard_scale.png)

위의 데이터셋을 보면 StandardScaler에 의해 모든 변수의 평균은 0, 표준편차는 1로 변환된것을 볼 수 있다. 이제 Robust Scale로 변환해보자.

### 2. Robust Scale

Robust Scale은 중앙값(median)dl 0, IQR(interquartile range)이 1이 되도록 변환해준다. Roubust Scale은 이상치의 영향을 최소화한 기법으로
Standard Scale보다 표준화 값을 더 넓게 분포시킨다는 특징이 있다.

```python
from sklearn.preprocessing import StandardScaler

scale_data = data.iloc[:, range(0, len(data.columns)-2)]

scaler = RobustScaler()
scaler.fit(scale_data)
scale_data_scaled_robust = robust_scaler.transform(scale_data)
scale_data_scaled_robust = pd.DataFrame(scale_data_scaled_robust).describe()
scale_data_scaled_robust.columns = scale_data.columns
scale_data_scaled_robust = round(scale_data_scaled_robust)
```

![](/img/seoulpark_series/robust_scale.png)

위의 Roubust Sclae을 보면 중앙값인 '50%'가 0임을 알 수 있고 '25%'~'75%'까지의 거리인 IQR이 1임을 알 수 있다. 두 스케일 변환의 가장 큰 차이점은
분산인 'std'인데 StandardSclaer로 변환한 것과는 다르게 RobustScaler로 변환된 값에서는 유독 강우량(rainfall)의 분산이 매우 크게 나타남을 알 수 있다.
그렇다면 이상치 제거 과정을 통해 위의 두 Scale 중 어느것을 사용하는게 더 적절한지 살펴보자.

## Isolation Forest

Isolation Forest는 Regression Tree 기반으로 데이터를 계속 Split하여 데이터 관측치를 고립시키는 방법이다. 이상치일 수록 데이터가 크게 벗아나 있기 때문에 Split하는 과정에서
빨리 고립된다. 이와 반대로 정상적인 데이터는 데이터 범위 내에 있기 때문에 Split과정을 여러번 거친 후에야 고립된다. 이를 그림으로 보면 쉽게 이해할 수 있다.

![](/img/seoulpark_series/isolation.png)

위의 그림을 보면 이상치인 X<sub>0</sub>는 데이터 범위에서 크게 벗어나 있기 때문에 몇번의 Split만으로도 쉽게 고립됨을 알 수 있다. 이와 반면에 정상치인 X<sub>i</sub>는 데이터 범위 내에 있기 때문에
많은 Split과정을 거쳐야 고립됨을 볼 수 있다.

### 1. Isolation Forest을 왜 사용할까?

1. 다차원의 데이터셋에서 효율적으로 작동하는 이상치 제거 방법
2. 의사결정트리 기반의 이상치 탐지 기법
3. 의사결정트리의 샘플 분포를 여러개 뽑아 앙상블 모델로 변환하여 안정적인 이상치 탐지 및 제거 가능
4. 군집기반 이상탐지 알고리즘에 비해 월등한 실행 성능
5. 전체 데이터가 아닌 데이터를 샘플링해서 사용하기 때문에 'Swamping'가 'Masking' 문제를 예방할 수 있다.
> 'Swamping'
정상치가 이상치에 가까운 경우 정상치를 이상치로 잘못 분류하는 현상
> 'Masking'
이상치가 군집화되어 있으면 정상치로 잘못 분류하는 현상


### 2. Isolation을 통한 이상치 제거

```python
from sklearn.ensemble import IsolationForest
import collections

# n_estimators : 노드 수 (50 - 100사이의 숫자면 적당하다.)
# max_samples : 샘플링 수
# contamination : 이상치 비율
# max_features : 사용하고자 하는 독립변수 수 (1이면 전부 사용)
# random_state : seed를 일정하게 유지시켜줌(if None, the random number generator is the RandomState instance used by np.random)
# n_jobs : CPU 병렬처리 유뮤(1이면 안하는 것으로 디버깅에 유리. -1을 넣으면 multilple CPU를 사용하게 되어 메모리 사용량이 급격히 늘어날 수 있다.)

clf = IsolationForest(n_estimators=100,
                      max_samples="auto", 
                      contamination=0.1,
                      max_features=1,
                      bootstrap=False,
                      n_jobs=1,
                      random_state=None,
                      verbose=0)

# fit 함수를 이용하여, 데이터셋을 학습시킨다.
clf.fit(scale_data)

# predict 함수를 이용하여, outlier를 판별해 준다. 0과 1로 이루어진 Series형태의 데이터가 나온다.
y_pred_outliers = clf.predict(scale_data)

# 이상치의 개수를 Count하는 과정
collections.Counter(y_pred_outliers)

# 원래의 DataFrame에 붙히기. out행의 값이 -1인 것을 제거하면 이상치가 제거된 DataFrame을 얻을 수 있다.
scale_data['out']=y_pred_outliers
outliers=scale_data.loc[scale_data['out']== -1]
outlier_index=list(outliers.index)
```

### 3. PCA로 Isolation Forest결과 확인하기.

PCA과정을 통해 Isolation Forest의 결과를 확인하는 과정이다. PCA를 사용하기 때문에 정규화된 데이터를 사용했다.

#### 3-1. RobustScaler, StandardScale를 PCA에 적용하기

```python
scale_data_numeric = scale_data.iloc[:, range(0, len(scale_data.columns)-1)]

# 1. 정규화_Robust

from sklearn.preprocessing import RobustScaler
 
robust_scaler = RobustScaler()
robust_scaler.fit(sub_eda_data)
X_scaled = robust_scaler.transform(sub_eda_data)

# 2. 정규화_Standard

scaler = StandardScaler()
scaler.fit(sub_eda_data)
X_scaled = scaler.transform(sub_eda_data)

# 3. PCA(component = 3), Scaler와 무관하게 코드 동일

from mpl_toolkits.mplot3d import Axes3D
from sklearn.decomposition import PCA

pca = PCA(n_components=3)
X_scaled_reduce = pca.fit_transform(X_scaled)
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.set_zlabel("x_composite_3")
# Plot the compressed data points
ax.scatter(X_scaled_reduce[:, 0], X_scaled_reduce[:, 1], zs=X_scaled_reduce[:, 2], s=4, lw=4, label="inliers",c="green")
# Plot x's for the ground truth outliers
ax.scatter(X_scaled_reduce[outlier_index,0],X_scaled_reduce[outlier_index,1], 
           X_scaled_reduce[outlier_index,2],
           lw=2, s=60, marker="x", c="red", label="outliers")
ax.legend()
plt.show()

# 4. PCA(component = 2), Scaler와 무관하게 코드 동일

pca = PCA(2)
pca.fit(X_scaled)
res=pd.DataFrame(pca.transform(X_scaled))
Z = np.array(res)
plt.title("IsolationForest")
b1 = plt.scatter(res[0], res[1], c='green',
                 s=20,label="normal points")
b1 =plt.scatter(res.iloc[outlier_index,0],res.iloc[outlier_index,1], c='green',s=20,  edgecolor="red",label="predicted outliers")
plt.legend(loc="upper right")
plt.show()
```

<!-- 결과 바뀌어서 파일명이랑 결과랑 반대로 봐야함 -->

##### RobustScaler 결과
![](/img/seoulpark_series/robust.png)

##### StandardScaler 결과

![](/img/seoulpark_series/isolation_res.png)

우선 동일한 데이터로 Isolation Forest를 사용했기에, 이상치는 Scaler와 무관하게 동일하다. 그ㄴ러나 PCA를 진행하는 과정에서 RobustScaler를 사용했을 때, 강우량의 분산이 너무 커서
PCA결과가 한쪽으로 치우쳐져 나오는 것을 알 수 있다.

그렇기 때문에 StandardScaler를 사용한 PCA결과를 살펴보면 components가 3인 PCA에서는 이상치가 잘 보이지 않지만 component가 2인 PCA에서는 이상치 탐지가 꽤 잘 이루어진 것을 알 수 있다.
이제 이상치 제거를 끝냈으니 다음 포스팅에서는 변수 선택에 대해 살펴보겠다.

<!-- ## *Series*

* ['1편-사라지는 도심 속 공원에 대한 해결책'](https://minkithub.github.io/minki.github.io/%EB%8D%B0%EC%9D%B4%ED%84%B0%20%EC%B9%BC%EB%9F%BC/seoulpark/) -->

## *reference*

* 서울시 공원일몰제 시행에 대비한 수직빌딩숲 입지 선정 및 제안 (김민기, 유민형, 2018)
* [donghwa-kim.github.io_Isolation Forest](https://donghwa-kim.github.io/iforest.html)
* [ko.logpresso.com](https://ko.logpresso.com/documents/anomaly-detection)
* [datascienceschool.net](https://datascienceschool.net/view-notebook/f43be7d6515b48c0beb909826993c856/)
